# Azure OpenAI Setup Guide

## ðŸŽ¯ **Why Azure OpenAI + Your Testing UI = Perfect Solution**

Instead of struggling with 20B models on 32GB RAM, you get:
- âœ… **GPT-3.5 Turbo** - the model you already know works great
- âœ… **GPT-4** - even better quality when needed  
- âœ… **No memory limits** - runs on Azure's infrastructure
- âœ… **Same testing UI** - compare local vs Azure models

---

## ðŸ”§ **Step 1: Create Azure OpenAI Resource**

### **Azure Portal Setup:**
1. **Go to**: [Azure Portal](https://portal.azure.com)
2. **Click**: "Create a resource"
3. **Search**: "Azure OpenAI" 
4. **Select**: "Azure OpenAI" by Microsoft
5. **Click**: "Create"

### **Configuration:**
```
Resource Group: ocp10 (same as your compute instance)
Name: ocp10-openai
Region: East US (or same as your compute)
Pricing Tier: Standard S0
```

### **â³ Wait for Approval** (Usually 1-2 business days)
Microsoft manually reviews Azure OpenAI requests.

---

## ðŸ”§ **Step 2: Deploy Models**

Once approved:

### **Deploy GPT-3.5 Turbo:**
1. **Go to**: Azure OpenAI Studio
2. **Click**: "Deployments" 
3. **Click**: "Create new deployment"
4. **Select**: 
   - Model: `gpt-35-turbo`
   - Deployment name: `gpt-35-turbo`
   - Version: Latest
5. **Click**: "Create"

### **Optional: Deploy GPT-4**
- Same process, select `gpt-4` model
- More expensive but higher quality

---

## ðŸ”§ **Step 3: Get Connection Details**

### **Find Your Endpoint:**
```
Azure Portal â†’ Your OpenAI Resource â†’ Overview
Copy: "Endpoint" URL
Example: https://ocp10-openai.openai.azure.com/
```

### **Get API Key:**
```
Azure Portal â†’ Your OpenAI Resource â†’ Keys and Endpoint
Copy: "KEY 1" 
```

---

## ðŸ”§ **Step 4: Configure Your Backend**

### **SSH to Your Compute Instance:**
```bash
az ml compute connect --name "multi-model-test" --resource-group ocp10 --workspace-name ocp10
```

### **Set Environment Variables:**
```bash
# Add to ~/.bashrc or set temporarily
export AZURE_OPENAI_KEY="your-api-key-here"
export AZURE_OPENAI_ENDPOINT="https://ocp10-openai.openai.azure.com/"
export AZURE_OPENAI_VERSION="2024-02-15-preview"

# Make permanent (optional)
echo 'export AZURE_OPENAI_KEY="your-api-key"' >> ~/.bashrc
echo 'export AZURE_OPENAI_ENDPOINT="https://ocp10-openai.openai.azure.com/"' >> ~/.bashrc
echo 'export AZURE_OPENAI_VERSION="2024-02-15-preview"' >> ~/.bashrc
source ~/.bashrc
```

### **Test Configuration:**
```bash
cd model-testing-ui/backend
python azure_openai_models.py
```

Expected output:
```
âœ… Azure OpenAI configured correctly!
Load test: Selected GPT-3.5 Turbo - ready for API calls
âœ… Generation test successful!
Response: This is a test message generated by GPT-3.5...
Cost: $0.0012
```

---

## ðŸ”§ **Step 5: Run Hybrid Backend**

### **Install Additional Dependencies:**
```bash
pip install openai
```

### **Start Hybrid Backend:**
```bash
python app_hybrid.py
```

Expected output:
```
ðŸš€ Starting Hybrid Model Testing UI
==================================================
Azure OpenAI configured: True
Local models available: 6
Azure models available: 2
* Running on all addresses (0.0.0.0)
* Running on http://localhost:5000
```

---

## ðŸŽ¨ **Step 6: Use Your Testing UI**

### **Now You Can Compare:**
- **Local Models**: GPT-2 variants, FLAN-T5, Phi-2
- **Azure Models**: GPT-3.5 Turbo, GPT-4
- **Side by side**: Same prompts, different models

### **Perfect for Grant Writing:**
1. **Test prompt**: "Write a grant application response..."
2. **Compare**:
   - GPT-2 XL (local, free, 8GB RAM)
   - FLAN-T5 XL (local, free, 12GB RAM) 
   - GPT-3.5 Turbo (Azure, $0.50/1K tokens, excellent)
   - GPT-4 (Azure, $15/1K tokens, premium)

---

## ðŸ’° **Cost Analysis**

### **Grant Writing Workload:**
- **Input**: 200 tokens (prompt)
- **Output**: 800 tokens (response)
- **Total**: 1000 tokens per test

### **Costs Per Test:**
- **GPT-3.5 Turbo**: $0.50 
- **GPT-4**: $15.00
- **Local models**: $0 (just compute time)

### **Monthly Budget Estimate:**
- **100 tests/month**: $50 (GPT-3.5) or $1,500 (GPT-4)
- **Compare with local**: Find the sweet spot

---

## ðŸŽ¯ **Recommended Testing Strategy**

### **Phase 1: Baseline (Free)**
1. **FLAN-T5 XL** - Best local model for structured tasks
2. **Phi-2** - Excellent reasoning for size
3. **GPT-2 XL** - Reliable baseline

### **Phase 2: Premium Comparison** 
4. **GPT-3.5 Turbo** - Industry standard
5. **GPT-4** - Premium quality

### **Result:** 
Find the model that gives you the best **quality/cost ratio** for grant writing.

---

## ðŸš¨ **If Azure OpenAI Request is Denied**

### **Alternatives:**
1. **OpenAI Direct API** (non-Azure)
   ```bash
   export OPENAI_API_KEY="your-openai-key"
   # Modify code to use OpenAI instead of Azure OpenAI
   ```

2. **Use Your Current Setup**
   - FLAN-T5 XL is actually excellent for grant writing
   - Phi-2 has surprisingly good reasoning
   - Much cheaper than API costs

---

## ðŸŽ‰ **Success Metrics**

### **Working Setup:**
âœ… Azure OpenAI resource created  
âœ… GPT-3.5 Turbo deployed  
âœ… API calls working from your UI  
âœ… Side-by-side comparison with local models  
âœ… Cost tracking per request  

### **Perfect Solution:**
You get the **exact same GPT-3.5 Turbo** you know works well, integrated into your model testing UI, while still being able to compare with free local alternatives.

**This is the optimal approach!** ðŸš€

---

## ðŸ†˜ **Troubleshooting**

### **Common Issues:**

**"Access denied" for Azure OpenAI:**
- Wait for Microsoft approval (can take days)
- Try different region
- Ensure you have proper Azure subscription

**"Model not found":**
- Make sure you deployed the model in Azure OpenAI Studio
- Check deployment name matches your code

**"Authentication failed":**
- Verify API key is correct
- Check endpoint URL format
- Ensure environment variables are set

**High costs:**
- Monitor token usage in your UI
- Set up Azure billing alerts
- Use GPT-3.5 instead of GPT-4 for testing